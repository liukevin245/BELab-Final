{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter import *\n",
    "import pandas as pd\n",
    "\n",
    "root = Tk()\n",
    "var = IntVar()\n",
    "button = Checkbutton(root, text='not confusing -> unpressed / confusing -> pressed', onvalue=1, offvalue=0, variable=var)\n",
    "check_flag, save_idx = False, 56\n",
    "label = eval(input('Give initial label: 0: not confusing / 1: confusing'))\n",
    "\n",
    "def check_label():\n",
    "    save_file = True\n",
    "    global var, label, check_flag, save_idx\n",
    "    button_value = var.get()\n",
    "    \n",
    "    training_data = pd.read_csv('training_data.csv')[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']]\n",
    "    if int(training_data['Delta'][0]) == int(check_flag):\n",
    "        save_file = False\n",
    "    elif button_value != label:\n",
    "        save_file = False\n",
    "        label = button_value\n",
    "        check_flag = not check_flag\n",
    "        \n",
    "    if save_file:\n",
    "        check_flag = not check_flag\n",
    "        training_data = training_data.drop([0])\n",
    "        labels = [label] * (len(training_data['Delta']) + 1)\n",
    "        label_df = pd.DataFrame({'Label': labels})\n",
    "        training_data = training_data.join(label_df)\n",
    "        training_data.to_csv(f'training_data_{save_idx}.csv')\n",
    "        save_idx += 1\n",
    "    \n",
    "    button.after(2000, check_label)\n",
    "\n",
    "button.pack()\n",
    "button.after(1000, check_label)\n",
    "root.update_idletasks()\n",
    "root.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Training Data\n",
    "\n",
    "Assume all files include the same number of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('training_data_26.csv', encoding='utf-8')[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2', 'Label']]\n",
    "timepoint = pd.DataFrame({'timepoint': [time for time, _ in enumerate(df['Delta'])]})\n",
    "df = timepoint.join(df)\n",
    "\n",
    "# allows 1000 files at most\n",
    "for i in range(27, 1000):\n",
    "    csv_path = f'training_data_{i}.csv'\n",
    "    if not csv_path in os.listdir('.'):\n",
    "        break\n",
    "    df_ = pd.read_csv(csv_path, encoding='utf-8')[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2', 'Label']]\n",
    "    df_ = timepoint.join(df_)\n",
    "    df = pd.concat([df, df_])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,15))\n",
    "cor_matrix = df.corr()\n",
    "sns.heatmap(cor_matrix, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe EEG Wave Difference for Confusing / Non-Confusing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusing_wave_plot(df_confused, df_understand):\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(30,8.27)})\n",
    "    fig, axs = plt.subplots(4, 2)\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Alpha1\",errorbar=None, ax=axs[0,0])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Alpha1\",errorbar=None, ax=axs[0,0])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Alpha2\",errorbar=None, ax=axs[0,1])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Alpha2\",errorbar=None, ax=axs[0,1])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Beta1\",errorbar=None, ax=axs[1,0])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Beta1\",errorbar=None, ax=axs[1,0])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Beta2\",errorbar=None, ax=axs[1,1])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Beta2\",errorbar=None, ax=axs[1,1])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Gamma1\",errorbar=None, ax=axs[2,0])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Gamma1\",errorbar=None, ax=axs[2,0])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Gamma2\",errorbar=None, ax=axs[2,1])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Gamma2\",errorbar=None, ax=axs[2,1])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Delta\",errorbar=None, ax=axs[3,0])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Delta\",errorbar=None, ax=axs[3,0])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Theta\",errorbar=None, ax=axs[3,1])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Theta\",errorbar=None, ax=axs[3,1])\n",
    "\n",
    "    fig.legend(labels=['confused','not confused'], loc=\"lower center\", ncol=2)\n",
    "    fig.suptitle('Confused vs Not Confused')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Label == 1'\n",
    "df_confused = df.query(query)\n",
    "\n",
    "query = 'Label == 0'\n",
    "df_understand = df.query(query)\n",
    "\n",
    "confusing_wave_plot(df_confused, df_understand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusing_wave_plot_v(df_confused, df_understand):\n",
    "    \n",
    "    sns.set(rc={'figure.figsize':(20,20)})\n",
    "    fig, axs = plt.subplots(8, 1)\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Alpha1\",errorbar=None, ax=axs[0])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Alpha1\",errorbar=None, ax=axs[0])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Alpha2\",errorbar=None, ax=axs[1])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Alpha2\",errorbar=None, ax=axs[1])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Beta1\",errorbar=None, ax=axs[2])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Beta1\",errorbar=None, ax=axs[2])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Beta2\",errorbar=None, ax=axs[3])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Beta2\",errorbar=None, ax=axs[3])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Gamma1\",errorbar=None, ax=axs[4])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Gamma1\",errorbar=None, ax=axs[4])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Gamma2\",errorbar=None, ax=axs[5])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Gamma2\",errorbar=None, ax=axs[5])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Delta\",errorbar=None, ax=axs[6])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Delta\",errorbar=None, ax=axs[6])\n",
    "\n",
    "    sns.lineplot(data=df_confused, x=\"timepoint\", y=\"Theta\",errorbar=None, ax=axs[7])\n",
    "    sns.lineplot(data=df_understand, x=\"timepoint\", y=\"Theta\",errorbar=None, ax=axs[7])\n",
    "\n",
    "    fig.legend(labels=['confused','not confused'], loc=\"lower center\", ncol=2)\n",
    "    fig.suptitle('Confused vs Not Confused')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Label == 1'\n",
    "df_confused = df.query(query)\n",
    "\n",
    "query = 'Label == 0'\n",
    "df_understand = df.query(query)\n",
    "\n",
    "confusing_wave_plot_v(df_confused, df_understand)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']]\n",
    "Y = df.Label\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "print(Y[Y == 0].shape, Y[Y == 1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, Y, save_path='./xgb_model_m.json', load=False):\n",
    "    random_state = 42\n",
    "\n",
    "    x_train,x_test,y_train,y_test = train_test_split(X, Y, test_size=0.2,\n",
    "                                    random_state=random_state, stratify=Y)\n",
    "    \n",
    "    if not load:\n",
    "        model = xgboost.XGBClassifier(base_score=0.5, learning_rate=0.1, max_depth=6, objective='binary:logistic', eta=0.01)\n",
    "    else:\n",
    "        model = xgboost.XGBClassifier()\n",
    "        model.load_model(save_path)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(x_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "\n",
    "    model.save_model(save_path)\n",
    "\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print('Accuracy: %.2f%%' % (accuracy * 100.0))\n",
    "\n",
    "    ## draw the tree\n",
    "    from xgboost import plot_tree\n",
    "    from matplotlib.pylab import rcParams\n",
    "\n",
    "    rcParams['figure.figsize'] = 80, 50\n",
    "\n",
    "    plot_tree(model)\n",
    "    plt.show()\n",
    "\n",
    "    ## show the cross validation result\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    results = cross_val_score(model, X, Y, cv=kfold)\n",
    "    print('Cross Validation Accuracy: %.2f%% (%.2f%%)' % (results.mean() * 100, results.std() * 100))\n",
    "    print('')\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print('')\n",
    "\n",
    "    # print feature importance\n",
    "    print('Feature Importance')\n",
    "    rcParams['figure.figsize'] = 5, 5\n",
    "    from xgboost import plot_importance\n",
    "    plot_importance(model)\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inference(X, model):\n",
    "    pred = model.predict(X)\n",
    "    confused = round(np.sum(pred) / len(pred))\n",
    "    \n",
    "    return confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from tkinter import *\n",
    "import xgboost\n",
    "import os\n",
    "\n",
    "root = Tk()\n",
    "root.title('Confusing Detector')\n",
    "screen_width = root.winfo_screenwidth() / 3\n",
    "screen_height = root.winfo_screenheight() / 2\n",
    "\n",
    "frame = Frame(root, width=root.winfo_screenwidth(), height=root.winfo_screenheight(), bg='green')\n",
    "\n",
    "print_num = 0\n",
    "prev_time = 0\n",
    "\n",
    "def main(model, eeg_path='./realtime_wave.csv'):\n",
    "    try:\n",
    "        global prev_time\n",
    "        curr_time = round(os.path.getmtime(eeg_path))\n",
    "        if curr_time == prev_time:\n",
    "            frame.after(500, main, model)\n",
    "            return\n",
    "        prev_time = curr_time\n",
    "        \n",
    "        # Step 1: read data\n",
    "        X = pd.read_csv(eeg_path)[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']]\n",
    "\n",
    "        # Step 2: preprocess data (modify units, ...)\n",
    "        # X *= 1600\n",
    "\n",
    "        # Step 3: inference it\n",
    "        result = inference(X, model)\n",
    "\n",
    "        # Step 4: print the result\n",
    "        print(result, end='')\n",
    "        global print_num\n",
    "        print_num += 1\n",
    "        if print_num % 80 == 0:\n",
    "            print()\n",
    "        \n",
    "        if not result:\n",
    "            frame['bg'] = 'green'\n",
    "        else:\n",
    "            frame['bg'] = 'red'\n",
    "\n",
    "        # Step 5: repeat these steps from time to time\n",
    "        frame.after(1000, main, model)\n",
    "    \n",
    "    except Exception:\n",
    "        frame.after(1000, main, model)\n",
    "\n",
    "model = xgboost.XGBClassifier()\n",
    "model.load_model('./xgb_model_m.json')\n",
    "\n",
    "frame.pack()\n",
    "frame.after(1000, main, model)\n",
    "root.update_idletasks()\n",
    "root.deiconify()\n",
    "root.withdraw()\n",
    "root.geometry('%dx%d+%d+%d' % (screen_width, screen_height, screen_width * 2 - 20, 10))\n",
    "\n",
    "root.deiconify()\n",
    "root.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - LSTM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization & split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_ma = df[['Delta','Theta','Alpha1','Alpha2','Beta1','Beta2','Gamma1','Gamma2']]\n",
    "Y_ma = df.Label\n",
    "\n",
    "x_lstm = StandardScaler().fit_transform(X_ma)\n",
    "y_lstm = Y_ma\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(x_lstm, y_lstm,test_size=0.2, random_state=42, stratify=y_lstm)\n",
    "\n",
    "\n",
    "n_features = X_ma.shape[1]\n",
    "x_train = np.array(x_train).reshape(-1,n_features,1)\n",
    "x_test = np.array(x_test).reshape(-1,n_features,1)\n",
    "\n",
    "x_train.shape, x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten, concatenate, Input, Dropout, LSTM, Bidirectional,BatchNormalization,PReLU,ReLU,Reshape\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow import keras\n",
    "\n",
    "inputs = tf.keras.Input(shape=(n_features,1))\n",
    "\n",
    "Dense1 = Dense(64, activation = 'relu',kernel_regularizer=keras.regularizers.l2())(inputs)\n",
    "\n",
    "lstm_1=  Bidirectional(LSTM(256, return_sequences = True))(Dense1)\n",
    "drop = Dropout(0.3)(lstm_1)\n",
    "lstm_3=  Bidirectional(LSTM(128, return_sequences = True))(drop)\n",
    "drop2 = Dropout(0.3)(lstm_3)\n",
    "\n",
    "flat = Flatten()(drop2)\n",
    "\n",
    "Dense_2 = Dense(128, activation = 'relu')(flat)\n",
    "outputs = Dense(1, activation='sigmoid')(Dense_2)\n",
    "\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,x_train, y_train,x_test,y_test, save_to, epoch):\n",
    "        opt_adam = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
    "        mc = ModelCheckpoint(save_to + 'best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "        lr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 0.001 * np.exp(-epoch / 10.))\n",
    "        \n",
    "        model.compile(optimizer=opt_adam,\n",
    "                  loss=['binary_crossentropy'],\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "        history = model.fit(x_train,y_train,\n",
    "                        batch_size=20,\n",
    "                        epochs=epoch,\n",
    "                        validation_data=(x_test,y_test),\n",
    "                        callbacks=[es,mc,lr_schedule],verbose=0)\n",
    "        \n",
    "        from matplotlib.pylab import rcParams\n",
    "        rcParams['figure.figsize'] = 5,5\n",
    "\n",
    "        plt.plot(history.history['accuracy'])\n",
    "        plt.plot(history.history['val_accuracy'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()\n",
    "        \n",
    "        return model,history\n",
    "        \n",
    "model,history = train_model(model, x_train, y_train,x_test, y_test, save_to= './', epoch = 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def inference(X, model):\n",
    "    pred = model.predict(X)[:, 0]\n",
    "#     print(pred.shape)\n",
    "    prob = np.sum(pred) / len(pred)\n",
    "    print(prob)\n",
    "    confused = round(prob)\n",
    "    \n",
    "    return confused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tkinter import *\n",
    "import os\n",
    "\n",
    "root = Tk()\n",
    "root.title('Confusing Detector')\n",
    "screen_width = root.winfo_screenwidth() / 3\n",
    "screen_height = root.winfo_screenheight() / 2\n",
    "\n",
    "frame = Frame(root, width=root.winfo_screenwidth(), height=root.winfo_screenheight(), bg='green')\n",
    "\n",
    "print_num = 0\n",
    "prev_time = 0\n",
    "\n",
    "def main(model, eeg_path='./realtime_wave.csv'):\n",
    "    try:\n",
    "        global prev_time\n",
    "        curr_time = round(os.path.getmtime(eeg_path))\n",
    "        if curr_time == prev_time:\n",
    "            frame.after(500, main, model)\n",
    "            return\n",
    "        prev_time = curr_time\n",
    "        \n",
    "        # Step 1: read data\n",
    "        X = pd.read_csv(eeg_path)[['Delta', 'Theta', 'Alpha1', 'Alpha2', 'Beta1', 'Beta2', 'Gamma1', 'Gamma2']]\n",
    "\n",
    "        # Step 2: preprocess data (modify units, ...)\n",
    "        X_lstm = StandardScaler().fit_transform(X)\n",
    "        n_features = X.shape[1]\n",
    "        X_inference = np.array(X_lstm).reshape(-1,n_features,1)\n",
    "#         print(X_inference.shape)\n",
    "\n",
    "        # Step 3: inference it\n",
    "        result = inference(X_inference, model)\n",
    "\n",
    "        # Step 4: print the result\n",
    "        print(result, end='')\n",
    "        global print_num\n",
    "        print_num += 1\n",
    "        if print_num % 80 == 0:\n",
    "            print()\n",
    "        \n",
    "        if not result:\n",
    "            frame['bg'] = 'green'\n",
    "        else:\n",
    "            frame['bg'] = 'red'\n",
    "\n",
    "        # Step 5: repeat these steps from time to time\n",
    "        frame.after(500, main, model)\n",
    "    \n",
    "    except Exception:\n",
    "        frame.after(500, main, model)\n",
    "\n",
    "inputs = tf.keras.Input(shape=(n_features,1))\n",
    "Dense1 = Dense(64, activation = 'relu',kernel_regularizer=keras.regularizers.l2())(inputs)\n",
    "lstm_1=  Bidirectional(LSTM(256, return_sequences = True))(Dense1)\n",
    "drop = Dropout(0.3)(lstm_1)\n",
    "lstm_3=  Bidirectional(LSTM(128, return_sequences = True))(drop)\n",
    "drop2 = Dropout(0.3)(lstm_3)\n",
    "flat = Flatten()(drop2)\n",
    "Dense_2 = Dense(128, activation = 'relu')(flat)\n",
    "outputs = Dense(1, activation='sigmoid')(Dense_2)\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "model.load_weights(\"./best_model.h5\")\n",
    "\n",
    "frame.pack()\n",
    "frame.after(1000, main, model)\n",
    "root.update_idletasks()\n",
    "root.deiconify()\n",
    "root.withdraw()\n",
    "root.geometry('%dx%d+%d+%d' % (screen_width, screen_height, screen_width * 2 - 20, 10))\n",
    "\n",
    "root.deiconify()\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
